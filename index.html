<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Supplementary Page</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Generated or Not?</h1>
        <p>Exploring the Distinction Between AI-Generated and Human-Composed Music</p>
    </header>

    <nav>
        <a href="#introduction">Introduction</a>
        <a href="#examples">Musical Examples</a>
        <a href="#technical">Technical Methods/Results</a>
        <a href="#class-survey">Class Study</a>
        <a href="#large-study">Larger Study</a>
    </nav>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <p>This project investigates the ability to predict whether a piece of music is AI-generated or human-composed Our analysis aims to identify key features that differentiate these compositions and explore how AI can impact the sound of music.</p>
        </section>

        <section id="examples">
            <h2>Musical Examples</h2>
            <p>Below are some examples of AI-generated and human-composed music from the datasets we are using that illustrate the problem we are solving:</p>
            <div class="audio-container">
                <div class="audio-item">
                    <h3>AIME: AI-Generated Samples</h3>
                    <audio controls>
                        <source src="audio/ai_generated/AIME_001.wav" type="audio/mpeg">
                        Your browser does not support the audio element.
                    </audio>
                    <audio controls>
                        <source src="audio/ai_generated/AIME_002.wav" type="audio/mpeg">
                        Your browser does not support the audio element.
                    </audio>
                    <audio controls>
                        <source src="audio/ai_generated/AIME_003.wav" type="audio/mpeg">
                        Your browser does not support the audio element.
                    </audio>
                    <audio controls>
                        <source src="audio/ai_generated/AIME_004.wav" type="audio/mpeg">
                        Your browser does not support the audio element.
                    </audio>
                </div>
                <div class="audio-item">
                    <h3>Music Caps: Human-Composed Samples</h3>
                    <audio controls>
                        <source src="audio/human_composed/MusicCaps_001.wav" type="audio/mpeg">
                        Your browser does not support the audio element.
                    </audio>
                    <audio controls>
                        <source src="audio/ai_generated/AIME_002.wav" type="audio/mpeg">
                        Your browser does not support the audio element.
                    </audio>
                    <audio controls>
                        <source src="audio/ai_generated/AIME_003.wav" type="audio/mpeg">
                        Your browser does not support the audio element.
                    </audio>
                    <audio controls>
                        <source src="audio/ai_generated/AIME_004.wav" type="audio/mpeg">
                        Your browser does not support the audio element.
                    </audio>
                </div>
            </div>
        </section>

        <section id="technical">
            <h2>Technical Methods and Results</h2>
            <div class="figure-container">
                <div class="figure-item">
                    <h3>Figure 1: t-SNE Visualization of CLAP Embeddings</h3>
                    <img src="images/embeddings_figures/t-SNE_AIME_MusicCaps.png" alt="t-SNE Visualization">
                </div>
            </div>
            <p>The t-SNE visualization reveals distinct clustering between AIME (blue) and MusicCaps (orange) embeddings, indicating that the audio representations from AI-generated and human-composed music occupy separable regions in the embedding space. While there is some overlap, particularly in regions with denser points, the differentiation suggests that the embeddings capture meaningful features that distinguish AI-generated music from human-composed music. This separability aligns with the classification results and highlights the effectiveness of the CLAP embeddings in capturing distinct compositional traits between the two datasets.</p>
            <div class="figure-container">
                <div class="figure-item">
                    <h3>Figure 2: t-SNE Visualization of CLAP Embeddings for AIME by Generative Model</h3>
                    <img src="images/embeddings_figures/generative_model.png" alt="t-SNE by Model Visualization">
                </div>
            </div>
            <p>The embedding visualization shows distinct clusters for models like Riffusion and Mustango, while significant overlap exists among versions of the same base model (e.g., MusicGen Small, Medium, Large). This overlap aligns with misclassifications in the confusion matrix and suggests similarities in embedding patterns due to shared architectures. The clustering raises questions about variations in output quality across models and their impact on classification tasks, including AI vs. human differentiation.</p>
        </section>

        <section id="class-survey">
            <h2>Results from the Class Survey</h2>
            <div class="figure-container">
                <div class="figure-item">
                    <h3>Figure 3: Media Lab Study Audio</h3>
                    <img src="images/class_study/ML_Audio_Pilot.png" alt="Media Lab Study Audio">
                </div>
                <div class="figure-item">
                    <h3>Figure 4: Dataset Audio</h3>
                    <img src="images/class_study/Dataset_Audio_Pilot.png" alt="Dataset Audio">
                </div>
            </div>
            <p>Our preliminary analysis of the class survey reveals the following insights:</p>
            <ul>
                <li>AI-generated music tends to have more repetitive patterns.</li>
                <li>Human-composed music demonstrates greater adherence to themes and more emotion.</li>
                <li>Listeners rated AI-generated music as "very generic."</li>
            </ul>
            <p>Our findings show that there is a struggle to distinguish between the pieces and we will further investigate the results as well as potentially run another larger scale study to learn more.</p>
        </section>

        <section id="large-study">
            <h2>Results from the Larger Study</h2>
            <div class="figure-container">
                <div class="figure-item">
                    <img src="images/class_study/ML_Audio_Pilot.png" alt="Media Lab Study Audio">
                    <h3>Figure 5: Large-scale Prolific Listening Study Results</h3>
                </div>
            </div>
            <p>The results above are the participant correct and incorrect classifications of the 12 seven-second-snippets of AIME and MusicCaps audio. Error bars indicate a standard deviation of 0.5025</p>
            <p>Our analysis of the study reveals the following insights:</p>
            <ul>
                <li>There were significant differences in the distribution of cor-rect and incorrect responses (Ï‡<sup>2</sup> = 26.667, df = 11, p = 0.00515)</li>
                <li>Post-hoc analysis using standardized residuals was performed to identify significant deviations from expected counts</li>
                <li>To account for multiple comparisons, p-values were adjusted using the Bonferroni correction. After adjustment, there was no significant difference among the classification responses.</li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; Fall 2024 Ananya Kulshrestha and Kimaya Lecamwasam | Contact: <a href="mailto:ananya_k@mit.edu">ananya_k@mit.edu</a> and <a href="mailto:klecamwa@mit.edu">klecamwa@mit.edu</a></p>
    </footer>
</body>
</html>

