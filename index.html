<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Supplementary Page</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Generated or Not?</h1>
        <p>Exploring the Distinction Between AI-Generated and Human-Composed Music</p>
    </header>

    <nav>
        <a href="#introduction">Introduction</a>
        <a href="#examples">Musical Examples</a>
        <a href="#results">Results</a>
        <a href="#conclusion">Conclusion</a>
    </nav>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <p>This project investigates the emotional resonance of AI-generated versus human-composed music. Our analysis aims to identify key features that differentiate these compositions and explore how AI can enhance emotional impact in music.</p>
        </section>

        <section id="examples">
            <h2>Musical Examples</h2>
            <p>Below are some examples of AI-generated and human-composed music that illustrate the problem we are solving:</p>
            <div class="audio-container">
                <div class="audio-item">
                    <h3>Example 1: AI-Generated</h3>
                    <audio controls>
                        <source src="audio/ai_generated/AIME_001.wav" type="audio/mpeg">
                        Your browser does not support the audio element.
                    </audio>
                </div>
                <div class="audio-item">
                    <h3>Example 2: Human-Composed</h3>
                    <audio controls>
                        <source src="audio/human_composed/MusicCaps_001.wav" type="audio/mpeg">
                        Your browser does not support the audio element.
                    </audio>
                </div>
            </div>
        </section>

        <section id="results">
            <h2>Results</h2>
            <p>Our preliminary analysis reveals the following insights:</p>
            <ul>
                <li>AI-generated music tends to have more repetitive patterns.</li>
                <li>Human-composed music demonstrates greater variability in dynamics and tempo.</li>
                <li>Listeners rated AI-generated music as "less authentic" but appreciated its adherence to prompt themes.</li>
            </ul>
            <div class="figure-container">
                <div class="figure-item">
                    <h3>Figure 1: Emotional Resonance Ratings</h3>
                    <img src="figure1.png" alt="Emotional Resonance Ratings">
                </div>
                <div class="figure-item">
                    <h3>Figure 2: Feature Comparison</h3>
                    <img src="figure2.png" alt="Feature Comparison">
                </div>
            </div>
        </section>

        <section id="conclusion">
            <h2>Conclusion</h2>
            <p>Our findings highlight the potential for AI-generated music to evolve with more nuanced training. Future work will refine these models and incorporate deeper listener feedback.</p>
        </section>
    </main>

    <footer>
        <p>&copy; Fall 2024 Ananya Kulshrestha and Kimaya Lecamwasam | Contact: <a href="mailto:ananya_k@mit.edu">ananya_k@mit.edu</a> and <a href="mailto:klecamwa@mit.edu">klecamwa@mit.edu</a></p>
    </footer>
</body>
</html>
